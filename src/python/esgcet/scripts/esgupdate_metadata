#!/usr/bin/env python

import sys
import os
import getopt

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from esgcet.publish import readDatasetMap
from esgcet.publish.utility import getTypeAndLen
from esgcet.config import loadConfig, initLogging, registerHandlers

from esgcet.exceptions import *
from esgcet.messaging import debug, info, warning, error, critical, exception
from esgcet.model import Dataset

usage = """Usage:
    esgupdate_metadata mapfile

    Update the local database with metadata defined in a mapfile. This is particularly useful
    for publishing checksums for existing datasets.

Arguments:
    mapfile: A file mapping dataset_ids to absolute paths, as generated by esgscan_directory.

Options:

    -v, --verbose
        Print messages.

"""

def main(argv):
    try:
        args, lastargs = getopt.getopt(argv, "v", ['verbose'])
    except getopt.error:
        print sys.exc_value
        print usage
        sys.exit(0)

    verbose = False
    for flag, arg in args:
        if flag in ['-v', '--verbose']:
            verbose = True

    if len(lastargs)!=1:
        print usage
        sys.exit(0)

    datasetMapfile = lastargs[0]
    init_file = None

    # Load the configuration and set up a database connection
    config = loadConfig(init_file)
    engine = create_engine(config.getdburl('extract'), echo=False, pool_recycle=3600)
    initLogging('extract', override_sa=engine)
    Session = sessionmaker(bind=engine, autoflush=True, autocommit=False)
    session = Session()

    # Register project handlers
    registerHandlers()

    # Read the mapfile: (dataset_name, version) => [(path, size), (path, size), ...]
    # and extraFields: (dataset_name, version, path, attribute) => attribute_value
    dmap, extraFields = readDatasetMap(datasetMapfile, parse_extra_fields=True)
    datasetNames = dmap.keys()
    datasetNames.sort()

    # Create a dictionary: (dataset_name, version, basename) => file_version_obj
    dsetdict = {}
    for datasetName, version in datasetNames:
        dset = session.query(Dataset).filter_by(name=datasetName).first()
        dsetVersion = dset.getVersionObj(version)
        filedict = {}
        for fvobj in dsetVersion.files:
            location = fvobj.getLocation()
            basename = os.path.basename(location)
            key = (datasetName, version, basename)
            if key in dsetdict:
                raise ESGPublishError("Duplicate file: %s, dataset=%s, version=%s"%(basename, datasetName, version))
            dsetdict[(datasetName, version, basename)] = fvobj

    # For each mapfile entry for the dataset:
    # - Find the corresponding file_version object
    # - Update the file_version
    for key, value in extraFields.items():
        datasetName, version, location, attrname = key
        basename = os.path.basename(location)
        fvobj = dsetdict[(datasetName, version, basename)]
        if attrname=='checksum':
            try:
                checksum_type = extraFields[(datasetName, version, location, 'checksum_type')]
            except KeyError:
                raise ESGPublishError("checksum_type not set for file=%s, dataset=%s, version=%s"%(location, datasetName, version))
            current_csum = fvobj.getChecksum()
            if current_csum not in (None, value):
                warning("checksum = %s for dataset=%s, file=%s, replacing with new checksum=%s"%(current_csum, datasetName, basename, value))
            if verbose:
                print "Setting checksum=%s, type=%s for dataset=%s, file=%s"%(value, checksum_type, datasetName, basename)
            fvobj.checksum = value
            fvobj.checksum_type = checksum_type
        else:
            continue

    session.commit()
    session.close()

if __name__=='__main__':
    main(sys.argv[1:])
