#!/usr/bin/env python

import sys
import os
import getopt

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from esgcet.publish import multiDirectoryIterator, directoryIterator, processIterator, readDatasetMap, processNodeMatchIterator, checksum, fnIterator
from esgcet.config import loadConfig, getHandler, getHandlerByName, initLogging, registerHandlers, getOfflineLister, splitLine
from esgcet.exceptions import *
from esgcet.messaging import debug, info, warning, error, critical, exception


usage = """Usage:
    esgscan_directory [options] directory [directory ...]

    Scan a set of directories recursively for a specific project, producing a dataset map of the form:

	dataset_id | absolute_file_path | size

    for each directory that has at least one file matching the file filter (see --filter).    

Arguments:
    directory: A directory to scan.

Options:

    -a dataset_map:
        Append the output to the existing dataset_map. If dataset_map does not exist, create it.
        Compare with -o.

    --dataset dataset_name:
        String name of the dataset. If specified, all files will belong to the specified dataset,
        regardless of path. See Notes for the behavior if omitted.

    --dataset-tech-notes url
        URL of the technical notes to be associated with each dataset.

    --dataset-tech-notes-title title
        Tech notes title for display.

    --filter regular_expression:
        Filter files matching the regular expression. Default is '.*\.nc$'
        Regular expression syntax is defined by the Python re module.

    -h, --help: Print a help message.

    -i init_file: Initialization file. If not specified, the default installed init file is read.

    -o, --output dataset_map:
        Name of the output dataset map file. If the file exists, replace it. Compare with -a.
        Defaults to standard output.

    --offline
        The directory is offline. The dataset name must also be specified with --dataset.

    -p, --property 'name=value':
        Add a property/value pair. This option can be used multiple times. The property is only
        used to generate the dataset ID.

        Note: the property must also be configured in the initialization file
        and project handler.

    --project project_id:
        Project identifier. If not specified, the project is determined from the first file found that
        matches the file filter.

    -e, --read-directories:
        Read dataset identification information from the directory
        names. THIS ASSUMES THAT EACH FILE IN A LEAF DIRECTORY BELONGS
        TO THE SAME DATASET. See --read-files, and Notes. This option
        is the default, and is generally faster than --read-files.

    --read-files:
        Read dataset identification information from each individual
        file. If not set, the dataset ID is generated by matching the
        directory with the config file option 'directory_format'.  See
        --read-directories and Notes.

    --service service
        Specify a THREDDS service name to associate with an offline dataset. If omitted, the name of the
        first offline service in the configuration ''thredds_offline_services'' is used. This determines
        which offline lister to use.

Notes:

    If --dataset is not set, the dataset IDs are generated in one of two ways:

    (1) If --read-files is set:

        (a) The directories are searched recursively. For each file matching the file filter:

        (b) The global attributes are read from the file metadata, by the project handler.

        (c) The dataset_id configuration option is used to generate the identifier, based on the file metadata.

        Example: If a CMIP5 file has attributes:

            :model_id = "cnrm-cm5" ;
            :experiment_id = "1pctCO2"

        and the dataset_id option is cmip5.%(model)s.%(experiment)s, then the dataset ID is 'cmip5.cnrm-cm5.1pctCO2'.

    (2) If --read-files is NOT set:

        (a) The directories are searched recursively. For each directory that contains a file matching
        the file filter:

        (b) The directory is compared against the directory_format configuration option for the project.
        If it matches:

        (c) The dataset_id configuration option is used to generate the identifier, based on the match.

        Example: If the directory_format = /data/%(model)s/%(experiment)s, and dataset_id = myproj.%(model)s.%(experiment)s,
        then files in directory /data/pcm/history will have dataset IDs 'myproj.pcm.history'.

        IMPORTANT: This option assumes that for a given leaf directory, each file in the directory belongs to the
        same dataset. If you are publishing CMIP5 data, and are using the publisher to recognize the 'product' value,
        it is NOT recommended to use this option, since for some files the 'product' depends on the time range
        of the file.

"""

def main(argv):
    try:
        args, lastargs = getopt.getopt(argv, "a:ehi:o:p:", ['dataset=', 'dataset-tech-notes=', 'dataset-tech-notes-title=', 'filter=', 'help', 'offline', 'output=', 'project=', 'property=', 'read-directories', 'read-files', 'service='])
    except getopt.error:
        print sys.exc_value
        print usage
        sys.exit(0)

    if len(lastargs)==0:
        print 'No directory specified'
        print usage
        sys.exit(0)

    appendMap = None
    datasetName = None
    datasetTechNotesURL = None
    datasetTechNotesTitle = None
    filefilt = '.*'
    init_file = None
    offline = False
    output = sys.stdout
    projectName = None
    properties = {}
    readFiles = False
    service = None
    for flag, arg in args:
        if flag=='-a':
            if os.path.exists(arg):
                appendMap = readDatasetMap(arg)
            else:
                appendMap = {}
            output = open(arg, 'a')
        elif flag=='--dataset':
            datasetName = arg
        elif flag=='--dataset-tech-notes':
            datasetTechNotesURL = arg
        elif flag=='--dataset-tech-notes-title':
            datasetTechNotesTitle = arg
        elif flag=='--filter':
            filefilt = arg
        elif flag in ['-h', '--help']:
            print usage
            sys.exit(0)
        elif flag=='-i':
            init_file = arg
        elif flag in ['-o', '--output']:
            output = open(arg, 'w')
        elif flag=='--offline':
            offline = True
        elif flag=='--project':
            projectName = arg
        elif flag in ['-p', '--property']:
            name, value = arg.split('=')
            properties[name] = value
        elif flag in ['-e', '--read-directories']:
            readFiles = False
        elif flag=='--read-files':
            readFiles = True
        elif flag=='--service':
            service = arg

    # Load the configuration and set up a database connection
    config = loadConfig(init_file)
    engine = create_engine(config.getdburl('extract'), echo=False, pool_recycle=3600)
    initLogging('extract', override_sa=engine)
    Session = sessionmaker(bind=engine, autoflush=True, autocommit=False)

    # Register project handlers
    registerHandlers()

    if not offline:

        # Determine if checksumming is enabled
        line = config.get('DEFAULT', 'checksum', default=None)
        if line is not None:
            checksumClient, checksumType = splitLine(line)
        else:
            checksumClient = None

        if projectName is not None:
            handler = getHandlerByName(projectName, None, Session)
        else:
            warning("No project name specified!")
            multiIter = multiDirectoryIterator(lastargs, filefilt=filefilt)
            firstFile, size = multiIter.next()
            handler = getHandler(firstFile, Session, validate=True)
            if handler is None:
                raise ESGPublishError("No project found in file %s, specify with --project."%firstFile)
            projectName = handler.name

        if not readFiles:
            datasetMap = handler.generateDirectoryMap(lastargs, filefilt, initContext=properties, datasetName=datasetName)
        else:
            datasetMap = handler.generateDirectoryMapFromFiles(lastargs, filefilt, initContext=properties, datasetName=datasetName)

        # Output the map
        keys = datasetMap.keys()
        keys.sort()
        for datasetId in keys:
            direcTuple = datasetMap[datasetId]
            direcTuple.sort()
            for nodepath, filepath in direcTuple:

                # If readFiles is not set, generate a map entry for each file in the directory
                # that matches filefilt ...
                if not readFiles:
                    itr = directoryIterator(nodepath, filefilt=filefilt, followSubdirectories=False)
                # ... otherwise if readFiles is set, generate a map entry for each file
                else:
                    itr = fnIterator([filepath])
                    
                for filepath, sizet in itr:
                    size, mtime = sizet
                    extraStuff = "mod_time=%f"%float(mtime)

                    if checksumClient is not None:
                        csum = checksum(filepath, checksumClient)
                        extraStuff += " | checksum=%s | checksum_type=%s"%(csum, checksumType)

                    if datasetTechNotesURL is not None:
                        extraStuff += " | dataset_tech_notes=%s"%datasetTechNotesURL
                        if datasetTechNotesURL is not None:
                            extraStuff += " | dataset_tech_notes_title=%s"%datasetTechNotesTitle

                    # Print the map entry if:
                    # - The map is being created, not appended, or
                    # - The existing map does not have the dataset, or
                    # - The existing map has the dataset, but not the file.
                    if (appendMap is None) or (not appendMap.has_key(datasetId)) or ((filepath, "%d"%size) not in appendMap[datasetId]):
                        print >>output, "%s | %s | %d | %s"%(datasetId, filepath, size, extraStuff)
    else:                               # offline
        if projectName is not None:
            handler = getHandlerByName(projectName, None, Session, offline=True)
        else:
            raise ESGPublishError("Must specify --project for offline datasets.")
        listerSection = getOfflineLister(config, "project:%s"%projectName, service)
        offlineLister = config.get(listerSection, 'offline_lister_executable')
        commandArgs = "--config-section %s "%listerSection
        commandArgs += " ".join(lastargs)
        for dsetName, filepath, sizet in processNodeMatchIterator(offlineLister, commandArgs, handler, filefilt=filefilt, datasetName=datasetName, offline=True):
            size, mtime = sizet
            extrastuff = ""
            if mtime is not None:
                extrastuff = "| mod_time=%f"%float(mtime)
            if (appendMap is None) or (not appendMap.has_key(dsetName)) or ((filepath, "%d"%size) not in appendMap[dsetName]):
                print >>output, "%s | %s | %d %s"%(dsetName, filepath, size, extrastuff)

    if output is not sys.stdout:
        output.close()

if __name__=='__main__':
    main(sys.argv[1:])
