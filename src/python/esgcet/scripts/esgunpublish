#!/usr/bin/env python

import sys
import os
import getopt
import logging
import string

from esgcet.model import *
from esgcet.publish import deleteDatasetList, updateThreddsMasterCatalog, reinitializeThredds, DELETE, UNPUBLISH, NO_OPERATION, UNINITIALIZED, readDatasetMap,\
    parseDatasetVersionId, generateDatasetVersionId, iterateOverDatasets, UPDATE_OP, publishDatasetList, updateThreddsMasterCatalog
from esgcet.config import loadConfig, initLogging, registerHandlers
from esgcet.exceptions import *
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, join
from esgcet.messaging import debug, info, warning, error, critical, exception
from esgcet.query import queryDatasetMap


usage = """Usage:
    esgunpublish [options] dataset_name [dataset_name ...]

    -or-

    esgunpublish [options] --map <dataset_mapfile or mapfile_directory>

    -or-

    esgunpublish [options] --rest-api 'dataset_name|data_node' ['dataset_name|data_node' ...]

    Delete or retract dataset(s) or specific versions of dataset(s) in this order:

      - Remove the dataset from the gateway (see --skip-gateway)
      - Remove the LAS entry (see --las)
      - Remove the THREDDS entry (see --skip-thredds)
      - Remove the local database entry, if --database-delete is set. By default the local database entry is
        kept intact, so that data files do not have to be rescanned.

    This script does not remove the data files, but does remove metadata associated
    with the dataset(s). The details of what metadata are removed depends on the options --gateway-retract
    and --database-delete.

    By default esgunpublish deletes all versions of the dataset. To unpublish a specific version n, specify
    the dataset as dataset_name#n.

    IMPORTANT: Note the difference in the way datasets are identified when the --rest-api option is used.
    With --rest-api, dataset IDs have the form 'dataset_name|data_node' where dataset_name includes
    the version number, for example, 'pcmdi.test.v1|pcmdi9.llnl.gov'. Using the other (non-RESTful) options,
    datasets do not include the data_node portion, for example 'pcmdi.test' or 'pcmdi.test#1'. If the wrong
    form is used, the dataset may not be deleted from the SOLR index correctly.

Arguments:
    dataset_name: The string name of the dataset(s) to be unpublished. To indicate a specific version,
    use the syntax dataset_name#n. If no version is indicated, the latest version is deleted.

Options:

    --delete
    --retract

    --database-delete: Delete the associated local database entry for this dataset. By default, the
      database information is left intact.

    --database-only: Short for --skip-gateway --skip-thredds --database-delete

    --echo-sql: Echo SQL commands

    -h, --help: Print a help message.

    -i init_dir:
        Directory containing all initialization files.
        Recommended: one initialization file for the default sections (esg.ini) and one per project, must match the name format esg.<project>.ini
        If not specified, the default installed init files are read.

    --las: Do not reinitialize the LAS server.

    --log log_file:
        Name of output log file. Overrides the configuration log_filename option. Default is standard output.

    --map dataset_mapfile or mapfile_directory: Delete all datasets in dataset_map. 'dataset_map' is a map file as generated by esgscan_directory.
        Use either a single mapfile as input or scan a directory recursively to unpublish all containing mapfiles.
        If this option is used, trailing command line arguments (dataset_names) are ignored.

    --no-republish: Do not republish the previous dataset version if the latest version is deleted. By default,
      if this option is not set, and the latest dataset version is deleted, the most recent version will be
      rescanned and republished.

    --no-thredds-reinit
        The THREDDS server is not reinitialized. This option is useful to remove one or more dataset catalogs
        without the overhead of a TDS reinitialization call, followed by a single reinitialization call.
        Use this option with caution, as it will leave the database and THREDDS catalogs in an inconsistent
        state.

    --rest-api
        Publish using the RESTful publication services. The configuration file option ``rest_service_url''
        defines the service location. If it is undefined, the service location is
        https://HOST/esg-search/ws, where HOST is derived from configuration option ``hessian_service_url''.

        Note: With this option, the dataset_name should have the form 'dataset_id|data_node'.

    --skip-index: Perform local (node) operations, skip SOLR index dataset delete.

    --skip-thredds: Do not remove associated THREDDS catalogs, or reinitialize the TDS server.
      By default, THREDDS catalogs are removed.

    --sync-thredds: Delete all THREDDS catalogs ('orphaned' catalogs) without a corresponding database entry.

    --use-list filelist
        Read the list of dataset names from a file, containing one
        dataset name per line. If the filelist is '-', read from
        standard input.

"""

def cleanupCatalogs(arg, dirname, names):
    for name in names:
        base, suffix = os.path.splitext(name)
        if suffix==".xml" and name not in ["catalog.xml"]:
            fullname = os.path.join(dirname, name)
            if not arg.has_key(fullname):
                ans = raw_input("The catalog %s is 'orphaned', delete? [y|n]: "%fullname)
                if ans.lower()=='y':
                    print "Deleting %s"%fullname
                    os.unlink(fullname)


def republishDataset(result, Session, thredds, gatewayOp):
    """
    Republish previous versions as needed. This will happen if the latest version
    was deleted from the database, and is not
    the only version. In this case the previous version will be rescanned to generate the aggregations.
    """
    statusDict, republishList = result
    if len(republishList)>0:
        registerHandlers() # Register project handlers.
        info("Republishing modified datasets:")
        republishDatasetNames = [generateDatasetVersionId(dsetTuple) for dsetTuple in republishList]
        dmap, offline, extraFields = queryDatasetMap(republishDatasetNames, Session, True)
        datasetNames = dmap.keys()
        datasets = iterateOverDatasets(None, dmap, None, republishList, Session, "time", UPDATE_OP, None, {}, offline, {}, forceAggregate=True, extraFields=extraFields)
        republishOp = (gatewayOp != NO_OPERATION) # Don't republish if skipping the gateway op
        result = publishDatasetList(datasetNames, Session, publish=republishOp, thredds=thredds)


def main(argv):

    try:
        args, lastargs = getopt.getopt(argv, "hi:", ['database-delete', 'database-only', 'echo-sql', 'map=', 'no-republish', 'no-thredds-reinit', 'skip-gateway', 'skip-index', 'las', 'log=', 'rest-api', 'skip-thredds', 'sync-thredds', 'use-list='])
    except getopt.error:
        print sys.exc_value
        print usage
        sys.exit(0)

    deleteAll = False
    datasetMap = None
    mapfileDir = None
    deleteDset = False
    unpublishOnGateway = False
    echoSql = False
    init_dir = '/esg/config/esgcet/'
    gatewayOp = UNINITIALIZED
    las = False
    log_filename = None
    republish = True
    restApi = None
    thredds = True
    syncThredds = False
    useList = False
    threddsReinit = True
    for flag, arg in args:
        if flag=='--database-delete':
            deleteDset = True
        elif flag=='--database-only':
            gatewayOp = NO_OPERATION
            thredds = False
            deleteDset = True
        elif flag=='--echo-sql':
            echoSql = True
        elif flag in ['-h', '--help']:
            print usage
            sys.exit(0)
        elif flag=='-i':
            init_dir = arg
        elif flag=='--map':
            if os.path.isfile(arg):
                datasetMap = readDatasetMap(arg)
            elif os.path.isdir(arg):
                mapfileDir = arg
            else:
                raise ESGPublishError("Not a valid file or directory: %s" % arg)
        elif flag=='--skip-gateway':
            gatewayOp = NO_OPERATION
        elif flag=='--skip-index':
            gatewayOp = NO_OPERATION
        elif flag=='--delete':
            gatewayOp = DELETE
        elif flag=='--retract':
            gatewayOp = RETRACT
        elif flag=='--las':
            las = True
        elif flag=='--log':
            log_filename = arg
        elif flag=='--no-republish':
            republish = False
        elif flag=='--no-thredds-reinit':
            threddsReinit = False
        elif flag=='--rest-api':
            restApi = True
        elif flag=='--skip-thredds':
            thredds = False
        elif flag=='--sync-thredds':
            syncThredds = True
        elif flag=='--use-list':
            useList = True
            useListPath = arg

    if gatewayOp!=NO_OPERATION and unpublishOnGateway:
        gatewayOp = UNPUBLISH

    # Load the configuration and set up a database connection
    config = loadConfig(init_dir)
    engine = create_engine(config.getdburl('extract'), echo=echoSql, pool_recycle=3600)
    initLogging('extract', override_sa=engine, log_filename=log_filename)
    Session = sessionmaker(bind=engine, autoflush=True, autocommit=False)

    if config is None:
        raise ESGPublishError("No configuration file found.")

    # Get the default publication interface (REST or Hessian)
    if restApi is None:
        restApi = config.getboolean('DEFAULT', 'use_rest_api', default=False)

    if datasetMap is None and mapfileDir is None:
        if not useList:
            datasetNames = [parseDatasetVersionId(item) for item in lastargs]
        else:
            if useListPath=='-':
                namelist = sys.stdin
            else:
                namelist = open(useListPath)
            datasetNames = []
            for line in namelist.readlines():
                versionId = parseDatasetVersionId(line.strip())
                datasetNames.append(versionId)
        result = deleteDatasetList(datasetNames, Session, gatewayOp, thredds, las, deleteDset, deleteAll=deleteAll, republish=republish, reinitThredds=threddsReinit, restInterface=restApi)

    elif mapfileDir is not None:
        for root, dirs, files in os.walk(mapfileDir, followlinks=True):
            for mapfile in files:
                try:
                    datasetMap = readDatasetMap(os.path.join(root, mapfile))
                except:
                    error("Skipping %s: File does not match ESGF mapfile format." %os.path.join(root, mapfile))
                    continue
                datasetNames = datasetMap.keys()
                datasetNames.sort()
                result = deleteDatasetList(datasetNames, Session, gatewayOp, thredds, las, deleteDset, deleteAll=deleteAll, republish=republish, reinitThredds=False, restInterface=restApi)

                if republish:
                    republishDataset(result, Session, thredds, gatewayOp)

        # reinitialize THREDDS catalog only once after processing all mapfiles
        if threddsReinit:
            updateThreddsMasterCatalog(Session)
            result = reinitializeThredds()

    else:
        datasetNames = datasetMap.keys()
        datasetNames.sort()

        result = deleteDatasetList(datasetNames, Session, gatewayOp, thredds, las, deleteDset, deleteAll=deleteAll, republish=republish, reinitThredds=threddsReinit, restInterface=restApi)

        if republish:
            republishDataset(result, Session, thredds, gatewayOp)

    # Synchronize database and THREDDS catalogs
    if syncThredds:
        threddsRoot = config.get('DEFAULT', 'thredds_root')

        # Make a dictionary of catalogs from the database
        session = Session()
        subcatalogs = session.query(Catalog).select_from(join(Catalog, Dataset, Catalog.dataset_name==Dataset.name)).all()
        catdict = {}
        for catalog in subcatalogs:
            location = os.path.join(threddsRoot, catalog.location)
            catdict[location] = 1
        session.close()

        # Scan all XML files in the threddsroot
        os.path.walk(threddsRoot, cleanupCatalogs, catdict)

if __name__=='__main__':
    main(sys.argv[1:])
