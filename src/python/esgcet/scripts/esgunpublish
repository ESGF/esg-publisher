#!/usr/bin/env python

import sys
import os
import getopt
import logging
import string

from esgcet.model import *
from esgcet.publish import deleteDatasetList, updateThreddsMasterCatalog, reinitializeThredds, DELETE, UNPUBLISH, NO_OPERATION, readDatasetMap, parseDatasetVersionId, generateDatasetVersionId, iterateOverDatasets, UPDATE_OP, publishDatasetList
from esgcet.config import loadConfig, initLogging, registerHandlers
from esgcet.exceptions import *
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, join
from esgcet.messaging import debug, info, warning, error, critical, exception
from esgcet.query import queryDatasetMap

usage = """Usage:
    esgunpublish [options] dataset_name [dataset_name ...]

    -or-

    esgunpublish [options] --map dataset_map

    Delete or retract dataset(s) or specific versions of dataset(s) in this order:

      - Remove the dataset from the gateway (see --skip-gateway)
      - Remove the LAS entry (see --las)
      - Remove the THREDDS entry (see --skip-thredds)
      - Remove the local database entry, if --database-delete is set. By default the local database entry is
        kept intact, so that data files do not have to be rescanned.
      
    This script does not remove the data files, but does remove metadata associated
    with the dataset(s). The details of what metadata are removed depends on the options --gateway-retract
    and --database-delete.

    By default esgunpublish deletes all versions of the dataset. To unpublish a specific version n, specify
    the dataset as dataset_name#n.

Arguments:
    dataset_name: The string name of the dataset(s) to be unpublished. To indicate a specific version,
    use the syntax dataset_name#n. If no version is indicated, the latest version is deleted.

Options:

    --database-delete: Delete the associated local database entry for this dataset. By default, the
      database information is left intact.

    --echo-sql: Echo SQL commands

    --gateway-retract: Retract gateway metadata about the dataset. The default is to 
      delete the dataset, which removes all metadata. In contrast, retracting makes
      the dataset undiscoverable from the portal, but leaves metrics and versioning information intact.
      Use --skip-gateway to suppress gateway metadata deletion.

    -h, --help: Print a help message.

    -i init_file: Initialization file. If not specified, the default installed init file is read.

    --las: Do not reinitialize the LAS server.

    --log log_file:
        Name of output log file. Overrides the configuration log_filename option. Default is standard output.

    --map dataset_map: Delete all datasets in dataset_map. 'dataset_map' is a map file
      as generated by esgscan_directory. If this option is used,
      trailing command line arguments (dataset_names) are ignored.

    --no-republish: Do not republish the previous dataset version if the latest version is deleted. By default,
      if this option is not set, and the latest dataset version is deleted, the most recent version will be
      rescanned and republished.

    --skip-gateway: Perform local (node) operations, skip gateway dataset delete. Ignore --gateway-retract option.

    --skip-thredds: Do not remove associated THREDDS catalogs, or reinitialize the TDS server.
      By default, THREDDS catalogs are removed.

    --sync-thredds: Delete all THREDDS catalogs ('orphaned' catalogs) without a corresponding database entry.
      
    --use-list filelist
        Read the list of dataset names from a file, containing one
        dataset name per line. If the filelist is '-', read from
        standard input.

"""

def cleanupCatalogs(arg, dirname, names):
    for name in names:
        base, suffix = os.path.splitext(name)
        if suffix==".xml" and name not in ["catalog.xml"]:
            fullname = os.path.join(dirname, name)
            if not arg.has_key(fullname):
                ans = raw_input("The catalog %s is 'orphaned', delete? [y|n]: "%fullname)
                if ans.lower()=='y':
                    print "Deleting %s"%fullname
                    os.unlink(fullname)

def main(argv):

    try:
        args, lastargs = getopt.getopt(argv, "hi:", ['database-delete', 'echo-sql', 'gateway-retract', 'map=', 'no-republish', 'skip-gateway', 'las', 'log=', 'skip-thredds', 'sync-thredds', 'use-list='])
    except getopt.error:
        print sys.exc_value
        print usage
        sys.exit(0)

    deleteAll = False
    datasetMap = None
    deleteDset = False
    unpublishOnGateway = False
    echoSql = False
    init_file = None
    gatewayOp = DELETE
    las = False
    log_filename = None
    republish = True
    thredds = True
    syncThredds = False
    useList = False
    for flag, arg in args:
        if flag=='--database-delete':
            deleteDset = True
        elif flag=='--echo-sql':
            echoSql = True
        elif flag=='--gateway-retract':
            unpublishOnGateway = True
        elif flag in ['-h', '--help']:
            print usage
            sys.exit(0)
        elif flag=='-i':
            init_file = arg
        elif flag=='--map':
            datasetMap = readDatasetMap(arg)
        elif flag=='--skip-gateway':
            gatewayOp = NO_OPERATION
        elif flag=='--las':
            las = True
        elif flag=='--log':
            log_filename = arg
        elif flag=='--no-republish':
            republish = False
        elif flag=='--skip-thredds':
            thredds = False
        elif flag=='--sync-thredds':
            syncThredds = True
        elif flag=='--use-list':
            useList = True
            useListPath = arg
            
    if gatewayOp!=NO_OPERATION and unpublishOnGateway:
        gatewayOp = UNPUBLISH

    # Load the configuration and set up a database connection
    config = loadConfig(init_file)
    engine = create_engine(config.getdburl('extract'), echo=echoSql, pool_recycle=3600)
    initLogging('extract', override_sa=engine, log_filename=log_filename)
    Session = sessionmaker(bind=engine, autoflush=True, autocommit=False)

    if config is None:
        raise ESGPublishError("No configuration file found.")
    threddsRoot = config.get('DEFAULT', 'thredds_root')

    if datasetMap is None:
        if not useList:
            datasetNames = [parseDatasetVersionId(item) for item in lastargs]
        else:
            if useListPath=='-':
                namelist = sys.stdin
            else:
                namelist = open(useListPath)
            datasetNames = []
            for line in namelist.readlines():
                versionId = parseDatasetVersionId(line.strip())
                datasetNames.append(versionId)
    else:
        datasetNames = datasetMap.keys()
        datasetNames.sort()
    result = deleteDatasetList(datasetNames, Session, gatewayOp, thredds, las, deleteDset, deleteAll=deleteAll, republish=republish)

    # Republish previous versions as needed. This will happen if the latest version
    # was deleted from the database, and is not
    # the only version. In this case the previous version will be rescanned to generate the aggregations.
    if republish:
        statusDict, republishList = result
        if len(republishList)>0:

            # Register project handlers.
            registerHandlers()

            info("Republishing modified datasets:")
            republishDatasetNames = [generateDatasetVersionId(dsetTuple) for dsetTuple in republishList]
            dmap, offline = queryDatasetMap(republishDatasetNames, Session)
            datasetNames = dmap.keys()
            datasets = iterateOverDatasets(None, dmap, None, republishList, Session, "time", UPDATE_OP, None, {}, offline, {}, forceAggregate=True)
            republishOp = (gatewayOp != NO_OPERATION) # Don't republish if skipping the gateway op
            result = publishDatasetList(datasetNames, Session, publish=republishOp, thredds=thredds)

    # Synchronize database and THREDDS catalogs
    if syncThredds:
        threddsRoot = config.get('DEFAULT', 'thredds_root')

        # Make a dictionary of catalogs from the database
        session = Session()
        subcatalogs = session.query(Catalog).select_from(join(Catalog, Dataset, Catalog.dataset_name==Dataset.name)).all()
        catdict = {}
        for catalog in subcatalogs:
            location = os.path.join(threddsRoot, catalog.location)
            catdict[location] = 1
        session.close()

        # Scan all XML files in the threddsroot
        os.path.walk(threddsRoot, cleanupCatalogs, catdict)

if __name__=='__main__':
    main(sys.argv[1:])
